# -*- coding: utf-8 -*-
"""감성 분류 모델 만들기.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rVlcxF_b9GCwTeokGD4kI5MZkrtL5gAk
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## 1. 데이터 로드 + 훈련데이터와 테스트 데이터로 나누기"""

cafe_reviews = pd.read_csv('C:/Users/User/Desktop/cafe_reviews.csv', encoding='utf-8') 
# 리뷰 4100개

from sklearn.model_selection import train_test_split 
# 훈련 데이터와 테스트 데이터를 랜덤으로 나누어 주기 위해 train_test_split import 해온다.
# 이 라이브러리를 사용하면 두 데이터에 0과 1을 골고루 섞어줄 수 있다.

train_data, test_data = train_test_split(cafe_reviews, random_state=42)

print(train_data.shape) # 훈련 데이터 3130개

print(test_data.shape) # 테스트 데이터 1044개

train_data = train_data[["review", "label"]] # 결측치 부분 제거
train_data

test_data = test_data[["review", "label"]]# 결측치 부분 제거
test_data

"""## 2. 데이터 정제하기"""

train_data['review'].nunique() # 데이터 중복 유무를 확인

train_data.drop_duplicates(subset=['review'], inplace=True) # 중복 샘플 제거

len(train_data) # 중복 샘플 제거 확인
# 2816이 나온 것으로 중복에 제거된 걸 알 수 있다.

train_data['label'].value_counts().plot(kind = 'bar')
print(train_data.groupby('label').size().reset_index(name = 'count')) 
# 0과 1이 골고루 있다.

print(train_data.isnull().values.any()) # Null이 있는가?

# Null이 존재한다. 어느 열에 있는걸까?
print(train_data.isnull().sum())

# label 열에 Null 존재
# 인덱스를 파악하자
train_data.loc[train_data.label.isnull()]

"""#### 친절하세요 감사합니다는 긍정이다. 데이터를 수정하고 앞의 과정을 다시 반복해주었다.

"""

print(train_data.isnull().values.any()) # Null이 있는지 다시 해본다. 
# False가 나와야 한다.

# 한글과 공백을 제외하고 모두 제거한다.
train_data['review'] = train_data['review'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
train_data[:5]

train_data['review'] = train_data['review'].str.replace('^ +', "") # white space 데이터를 empty value로 변경
train_data['review'].replace('', np.nan, inplace=True)
# 기존에 한글이 없는 리뷰였다면 더 이상 아무런 값도 없는 빈(empty) 값이 되었을 것
print(train_data.isnull().sum())

# Null 값이 10개 생김
train_data.loc[train_data.review.isnull()]

train_data = train_data.dropna(how = 'any') # Null 값 10개 없애준다.
print(len(train_data)) # 2816 => 2806

# 테스트 데이터에서도 똑같이 전처리 과정 수행

test_data.drop_duplicates(subset = ['review'], inplace=True) # review 열에서 중복인 내용이 있다면 중복 제거
test_data['review'] = test_data['review'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","") # 정규 표현식 수행
test_data['review'] = test_data['review'].str.replace('^ +', "") # 공백은 empty 값으로 변경
test_data['review'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경
test_data = test_data.dropna(how='any') # Null 값 제거
print('전처리 후 테스트 데이터의 개수 :',len(test_data))

"""## 3. 토큰화"""

# 불용어 정의
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', 'ㅠ', 'ㅠㅠ', 'ㅜㅜ', 'ㅋㅋ', 'ㅡㅡ', 'ㅠㅜ', 'ㄱ', 'ㄷㄷㄷ', 'ㅠㅠㅠㅠㅠㅠ', '❤']

# 형태소 분석기를 사용하여 토큰화를 하면서 불용어를 제거하여 X_train에 저장
X_train = []
for sentence in tqdm(train_data['review']):
    okt = Okt() # 형태소 분석기
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    X_train.append(stopwords_removed_sentence)

print(X_train[:3]) # 상위 3개

# 테스트 데이터에서도 동일하게 진행

X_test = []
for sentence in tqdm(test_data['review']):
    okt = Okt() # 형태소 분석기
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    X_test.append(stopwords_removed_sentence)

print(X_test[:3]) # 상위 3개

"""## 4. 정수 인코딩"""

# 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train) # 훈련 데이터에 대한 단어 집합 형성

print(tokenizer.word_index)

threshold = 3
total_cnt = len(tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :',total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

"""## => 희귀 단어 등장 빈도 비율이 9% 밖에 안되므로 등장 빈도수가 2 이하인 단어들을 제거한다."""

# 등장 빈도수가 2이하인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한

# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.
# 0번 패딩 토큰을 고려하여 + 1
vocab_size = total_cnt - rare_cnt + 1
print('단어 집합의 크기 :',vocab_size)

# 단어 집합의 크기를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환
tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

print(X_train[:3]) # 정수 인코딩 확인

# train_data에서 y_train과 y_test를 별도로 저장한다. 모델의 성능을 점검하게 위해서다.
y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])

"""## 5. 빈 샘플 제거"""

'''
전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 
-> 빈도수가 낮은 단어만으로 구성되었던 샘플들은 빈(empty) 샘플이 되었다는 것을 의미
-> 빈 샘플들을 제거해주는 작업 진행
'''

# 길이가 0인 샘플들의 인덱스를 받아오기
drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]

# drop_train에는 X_train으로부터 얻은 빈 샘플들의 인덱스가 저장됨
# 이제 빈 샘플들을 제거해보자.
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)
print(len(X_train))
print(len(y_train))

"""## 6. 패딩"""

# 서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 것이 패딩 작업

print('리뷰의 최대 길이 :',max(len(review) for review in X_train))
print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))
plt.hist([len(review) for review in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

def below_threshold_len(max_len, nested_list):
  count = 0
  for sentence in nested_list:
    if(len(sentence) <= max_len):
        count = count + 1
  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))

# 위의 분포 그래프를 봤을 때, max_len = 60이 적당할 것 같다.
max_len = 60
below_threshold_len(max_len, X_train)

# 길이가 60이하인 샘플의 비율이 97.7% => 모든 샘플의 길이를 60으로 통일하자
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

"""## 7. LSTM으로 카페 리뷰 감성 분류하기"""

from tensorflow.keras.layers import Embedding, Dense, LSTM
# Embedding: 벡터 공간에서 더 가까운 단어가 예상되도록 단어의 의미를 인코딩하는 실수 값 벡터의 형태로 텍스트 분석을 위한 단어 표현에 사용되는 용어
# Dense: 뉴런의 입력과 출력을 연결
# LSTM: RNN의 한 종류. 단순한 neural network layer 한 층 대신, 4개의 layer가  특별한 방식으로 서로 정보를 주고 받도록 되어 있다.
# LSTM -> 기울기 소멸 문제 방지

from tensorflow.keras.models import Sequential # 순차적으로 레이어 층 더해줌
from tensorflow.keras.models import load_model # 모델 불러오기 함수
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# EarlyStopping: 과적합을 방지하기 위해 조기종료
# ModelCheckpoint: 학습 중인 모델 자동으로 저장하기

embedding_dim = 100 # 임베딩 벡터 차원은 100
hidden_units = 128 # 은닉 상태의 크기는 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(1, activation='sigmoid')) # 활성화 함수로 시그모이드 함수 사용. 로지스틱 회귀를 사용해야 하므로
# 로지스틱 회귀분석에서는 종속변수가 0 또는 1이기 때문에  y=wx+b 을 이용해서 예측하는 것은 의미가 없다

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 조기종료
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 자동 저장

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) # 손실함수: binary_crossentropy. 이진 분류에 적합
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2) # 검증데이터는 20% 사용

loaded_model = load_model('best_model.h5') # 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'best_model.h5'를 로드
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1])) # 테스트 데이터에 대해서 정확도를 측정 => 모델 검증

"""## 테스트 정확도가 91%로, 좋은 모델이다.

---

## 8. 리뷰 예측 함수 만들기
"""

def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    print("{:.2f}% 확률로 긍정 리뷰입니다.\n".format(score * 100))
  else:
    print("{:.2f}% 확률로 부정 리뷰입니다.\n".format((1 - score) * 100))

sentiment_predict('맛있어요!')

sentiment_predict('공부하기 좋아요')

sentiment_predict('너무 시끄러워요')

sentiment_predict('벌레나오고 알바 싸가지 없음')

"""## Overview 만들어보기 - 스타벅스 대전 장대점"""

# 함수 재정의. (score * 100)% 확률로 긍정이다를 추출하도록 함

def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  print("{:.2f}% \n".format(score*100)) # (score * 100)% 확률로 긍정

starbucks_reviews = pd.read_csv('C:/Users/User/Desktop/starbucks_jangdaedong.csv', encoding='utf-8')

starbucks_reviews # 스타벅스 대전 장대점 리뷰 데이터

starbucks_reviews['review'].nunique() # 데이터 중복 유무 확인

starbucks_reviews.drop_duplicates(subset=['review'], inplace=True) # 중복 샘플 제거

# 한글과 공백을 제외하고 모두 제거한다.
starbucks_reviews['review'] = starbucks_reviews['review'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")

starbucks_reviews # 중복 샘플을 제거하여 인덱스가 엉켜있다.

starbucks_reviews = starbucks_reviews.reset_index() # 인덱스를 초기화해준다.
starbucks_reviews

for i in range(len(starbucks_reviews)): # 스타벅스 리뷰 데이터들을 모두 함수에 넣어 돌려본다.
    sentiment_predict(str(starbucks_reviews.iloc[i]))
    print(i)
    print("---------------------------------------------------------")

"""## => %가 높은 인덱스의 리뷰들, 즉 긍정적인 상위 5개 리뷰들을 추출한다. 그리고 이들을 합친다. 이것이 Overview

### 2번 인덱스(99.11%)
### 51번 인덱스(98.94%)
### 228번 인덱스(99.38%) 
### 120번 인덱스(99.61%)
### 231번 인덱스(99.04%)
### 위 다섯개 인덱스에 해당하는 리뷰들이 상위 5개의 리뷰로 뽑을 수 있다.

### 그러면 Overview는 다음과 같이 나온다.
"""

starbucks_reviews.iloc[2]['review']+" "+ starbucks_reviews.iloc[51]['review']+" "+starbucks_reviews.iloc[228]['review']+" "+starbucks_reviews.iloc[120]['review']+" "+starbucks_reviews.iloc[231]['review']

